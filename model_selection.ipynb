{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is the part of Georgetown University Data Science Project - Team Ship Happen\n",
    "\n",
    "\n",
    "## Purpose of this notebook is Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spaul\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Standard Python libraries\n",
    "import os                                    # For accessing operating system functionalities\n",
    "import json                                  # For encoding and decoding JSON data\n",
    "import pickle                                # For serializing and de-serializing Python objects\n",
    "\n",
    "# Libraries that can be pip installed\n",
    "import requests                              # Simple Python library for HTTP\n",
    "import pandas as pd                          # Library for building dataframes similar to those in R\n",
    "import seaborn as sns                        # Statistical visualization library based on Matplotlib\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV, LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, auc, roc_curve, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bunch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- data\n",
      "- feature_selection.ipynb\n",
      "- incident_knn-classifier.pickle\n",
      "- incident_random-forest-classifier.pickle\n",
      "- ingetion_wranling.ipynb\n",
      "- meta_incident.json\n",
      "- model evaluation.txt\n",
      "- model_selection.ipynb\n",
      "- model_selection_categorical.ipynb\n",
      "- mvinjury.txt\n",
      "- mvinjury_data.txt\n",
      "- mvinjury_data_final.txt\n",
      "- predicton comparision.xlsx\n",
      "- ReadMe.md\n",
      "- results_user_input_data_random-forest-classifier.txt\n",
      "- user_input_data.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = os.path.abspath(os.path.join(\".\", \"..\", \"Georgetown_Data_Science_Project\"))\n",
    "\n",
    "# Show the contents of the data directory\n",
    "for name in os.listdir(DATA_DIR):\n",
    "    if name.startswith(\".\"): continue\n",
    "    print(\"- {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260364, 6)\n",
      "(260364,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(root=DATA_DIR):\n",
    "    # Construct the `Bunch` for the Misle incident dataset\n",
    "    filenames     = {\n",
    "        'meta': os.path.join(root, 'meta_incident.json'),\n",
    "        'rdme': os.path.join(root, 'ReadMe.md'),        \n",
    "        'data': os.path.join(root, 'mvinjury_data_final.txt')        \n",
    "    }\n",
    "\n",
    "    # Load the meta data from the meta json\n",
    "    with open(filenames['meta'], 'r') as f:\n",
    "        meta = json.load(f)\n",
    "        target_names  = meta['target_names']\n",
    "        feature_names = meta['feature_names']\n",
    "\n",
    "    # Load the description from the README. \n",
    "    with open(filenames['rdme'], 'r') as f:\n",
    "        DESCR = f.read()\n",
    "\n",
    "    # Load the dataset from the text file.\n",
    "    mydataset = np.loadtxt(filenames['data'])\n",
    "\n",
    "    # Extract the target from the data\n",
    "    data   = mydataset[:, 0:-1]\n",
    "    target = mydataset[:, -1]\n",
    "\n",
    "    # Create the bunch object\n",
    "    return Bunch(\n",
    "        data=data,\n",
    "        target=target,\n",
    "        filenames=filenames,\n",
    "        target_names=target_names,\n",
    "        feature_names=feature_names,\n",
    "        DESCR=DESCR\n",
    "    )\n",
    "\n",
    "# Save the dataset as a variable we can use.\n",
    "mydataset = load_data()\n",
    "\n",
    "print(mydataset.data.shape)\n",
    "print(mydataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate(dataset, model, label, **kwargs):\n",
    "    \"\"\"\n",
    "    Because of the Scikit-Learn API, we can create a function to\n",
    "    do all of the fit and evaluate work on our behalf!\n",
    "    \"\"\"\n",
    "    start  = time.time() # Start the clock! \n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    \n",
    "    for train, test in KFold(mydataset.data.shape[0], n_folds=12, shuffle=True):\n",
    "        X_train, X_test = mydataset.data[train], mydataset.data[test]\n",
    "        y_train, y_test = mydataset.target[train], mydataset.target[test]\n",
    "        \n",
    "        estimator = model(**kwargs)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        expected  = y_test\n",
    "        predicted = estimator.predict(X_test)\n",
    "        \n",
    "        # Append our scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "\n",
    "    # Report\n",
    "    print(\"Build and Validation of {} took {:0.3f} seconds\".format(label, time.time()-start))\n",
    "    print(\"Validation scores are as follows:\\n\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Write official estimator to disk\n",
    "    estimator = model(**kwargs)\n",
    "    estimator.fit(mydataset.data, mydataset.target)\n",
    "    \n",
    "    outpath = label.lower().replace(\" \", \"-\") + \".pickle\"\n",
    "    with open(outpath, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "\n",
    "    print(\"\\nFitted model written to:\\n{}\".format(os.path.abspath(outpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform SVC Classification\n",
    "fit_and_evaluate(mydataset, SVC, \"Incident_SVM Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_KNN Classifier took 24.121 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.984910\n",
      "f1           0.977680\n",
      "precision    0.972103\n",
      "recall       0.984910\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\Georgetown_Data_Science_Project\\incident_knn-classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform kNN Classification\n",
    "fit_and_evaluate(mydataset, KNeighborsClassifier, \"Incident_KNN Classifier\", n_neighbors=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_Random Forest Classifier took 42.976 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.981422\n",
      "f1           0.977043\n",
      "precision    0.973395\n",
      "recall       0.981422\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\Georgetown_Data_Science_Project\\incident_random-forest-classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform Random Forest Classification\n",
    "fit_and_evaluate(mydataset, RandomForestClassifier, \"Incident_Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Take user data from text file to predict accident (yes/no), using random forest classifier model\n",
    "import csv\n",
    "\n",
    "def load_model(path='incident_random-forest-classifier.pickle'):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "# Create a reader for the text file and a write to write output \n",
    "with open('user_input_data.txt', 'r') as fin:\n",
    "    reader = csv.reader(fin, delimiter='\\t') \n",
    "\n",
    "    # Create writer to write CSV output \n",
    "    with open('results_user_input_data_random-forest-classifier.txt', 'w') as fout:\n",
    "        writer = csv.writer(fout) \n",
    "\n",
    "        # Go through all your data and run the predictions, writing to the results\n",
    "        for idx, row in enumerate(reader):\n",
    "            accident = model.predict([row]) \n",
    "            writer.writerow([idx+1,row[0], accident])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
